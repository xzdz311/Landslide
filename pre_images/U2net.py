import cv2

import random
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast  # æ··åˆç²¾åº¦è®­ç»ƒ
import numpy as np
from tqdm import tqdm
import os


# 3. è®¾ç½®å’Œå·¥å…·å‡½æ•°
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)


set_seed()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


# æ£€æŸ¥GPUæ•°é‡
def setup_multigpu():
    """è®¾ç½®å¤šGPUç¯å¢ƒ"""
    num_gpus = torch.cuda.device_count()
    print(f"æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU")

    if num_gpus > 1:
        print("å¯ç”¨å¤šGPUè®­ç»ƒ")
        # è®¾ç½®è®¾å¤‡ID
        device_ids = list(range(num_gpus))
        return device_ids
    else:
        print("å•GPUè®­ç»ƒ")
        return None


# 4. DEM è¯»å–å‡½æ•° (OpenCV æ›¿ä»£ rasterio)
def read_dem(file_path):
    """ä½¿ç”¨ OpenCV è¯»å– DEM æ–‡ä»¶ï¼Œå…¼å®¹å¤šç§æ ¼å¼"""
    dem = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)

    # å°è¯•æ›¿ä»£æ‰©å±•å
    if dem is None:
        base, ext = os.path.splitext(file_path)
        possible_exts = ['.tif', '.tiff', '.png', '.jpg', '.jpeg']
        for new_ext in possible_exts:
            alt_path = base + new_ext
            if os.path.exists(alt_path):
                dem = cv2.imread(alt_path, cv2.IMREAD_UNCHANGED)
                if dem is not None:
                    break

    # å¤„ç†è¯»å–å¤±è´¥
    if dem is None:
        return np.zeros((256, 256), dtype=np.float32)

    # ç¡®ä¿å•é€šé“
    if len(dem.shape) == 3:
        dem = dem[:, :, 0]  # å–ç¬¬ä¸€ä¸ªé€šé“

    return dem.astype(np.float32)


# 5. æ•°æ®é›†ç±»
class LandslideDataset(Dataset):
    def __init__(self, image_paths, dem_paths, mask_paths=None, transform=None, target_size=(256, 256)):
        self.image_paths = [p for p in image_paths if p is not None]
        self.dem_paths = [p for p in dem_paths if p is not None]

        # ç¡®ä¿mask_pathsåˆ—è¡¨é•¿åº¦ä¸image_pathsä¸€è‡´
        if mask_paths is None:
            self.mask_paths = [None] * len(self.image_paths)
        else:
            self.mask_paths = mask_paths

        self.transform = transform
        self.target_size = target_size

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # è¯»å–å…‰å­¦å›¾åƒ
        img_path = self.image_paths[idx]
        img = cv2.imread(img_path)
        if img is None:
            img = np.zeros((*self.target_size[::-1], 3), dtype=np.uint8)
        else:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # è¯»å–DEM
        dem_path = self.dem_paths[idx]
        dem = read_dem(dem_path)

        # è¯»å–æˆ–åˆ›å»ºæ©è†œ
        mask_path = self.mask_paths[idx]
        if mask_path is not None and os.path.exists(mask_path):
            # æœ‰çœŸå®maskï¼šè¯»å–å¹¶äºŒå€¼åŒ–
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            if mask is None:
                mask = np.zeros(self.target_size, dtype=np.uint8)
            else:
                _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
        else:
            # æ— æ»‘å¡æ ·æœ¬ï¼šåˆ›å»ºå…¨0çš„mask
            mask = np.zeros(self.target_size, dtype=np.uint8)

        # è°ƒæ•´å¤§å°
        img = cv2.resize(img, self.target_size, interpolation=cv2.INTER_LINEAR)
        dem = cv2.resize(dem, self.target_size, interpolation=cv2.INTER_NEAREST)
        mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)

        # å½’ä¸€åŒ–DEM
        dem_min, dem_max = dem.min(), dem.max()
        if dem_max > dem_min:
            dem = (dem - dem_min) / (dem_max - dem_min + 1e-8)
        else:
            dem = np.zeros_like(dem)

        # åˆå¹¶ä¸º4é€šé“
        dem = np.expand_dims(dem, axis=-1)
        combined = np.concatenate([img, dem], axis=-1)

        # åº”ç”¨å˜æ¢
        if self.transform:
            augmented = self.transform(image=combined, mask=mask)
            combined = augmented['image']
            mask = augmented['mask']

        # åˆ†ç¦»é€šé“
        optical = combined[:3, :, :]
        dem = combined[3:, :, :]
        mask = mask.unsqueeze(0).float() / 255.0

        # æ·»åŠ ä¸€ä¸ªæ ‡å¿—ä½ï¼šæ˜¯å¦ä¸ºæ»‘å¡æ ·æœ¬
        is_landslide = 1.0 if self.mask_paths[idx] is not None else 0.0

        return optical, dem, mask, is_landslide, img_path


# 6. æ•°æ®å‡†å¤‡å‡½æ•° (ä¿æŒä¸å˜ï¼Œä½†ç§»é™¤ rasterio ä¾èµ–)
def prepare_datasets_with_masks(data_dir, target_size=(256, 256), test_size=0.2):
    """
    å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†

    Args:
        data_dir: æ•°æ®é›†æ ¹ç›®å½•
        target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
        test_size: éªŒè¯é›†æ¯”ä¾‹
    """
    # ç¡¬ç¼–ç æ‰€æœ‰è·¯å¾„ - æ ¹æ®å®é™…ç›®å½•ç»“æ„
    landslide_train_image_dir = os.path.join(data_dir, 'landslide', 'train', 'image')  # å•æ•°
    landslide_train_mask_dir = os.path.join(data_dir, 'landslide', 'train', 'mask')
    landslide_train_dem_dir = os.path.join(data_dir, 'landslide', 'train', 'dem')

    landslide_test_image_dir = os.path.join(data_dir, 'landslide', 'test', 'images')  # å¤æ•°
    landslide_test_mask_dir = os.path.join(data_dir, 'landslide', 'test', 'mask')
    landslide_test_dem_dir = os.path.join(data_dir, 'landslide', 'test', 'dem')

    non_landslide_train_image_dir = os.path.join(data_dir, 'non-landslide', 'train', 'images')  # å¤æ•°
    non_landslide_train_dem_dir = os.path.join(data_dir, 'non-landslide', 'train', 'dem')

    non_landslide_test_image_dir = os.path.join(data_dir, 'non-landslide', 'test', 'images')  # å¤æ•°
    non_landslide_test_dem_dir = os.path.join(data_dir, 'non-landslide', 'test', 'dem')

    # æ”¶é›†æ»‘å¡è®­ç»ƒæ•°æ®
    landslide_train_imgs = []
    landslide_train_dems = []
    landslide_train_masks = []

    for img_file in os.listdir(landslide_train_image_dir):
        if img_file.lower().endswith('.png'):
            img_path = os.path.join(landslide_train_image_dir, img_file)
            dem_path = os.path.join(landslide_train_dem_dir, img_file)
            mask_path = os.path.join(landslide_train_mask_dir, img_file)

            if os.path.exists(img_path) and os.path.exists(dem_path) and os.path.exists(mask_path):
                landslide_train_imgs.append(img_path)
                landslide_train_dems.append(dem_path)
                landslide_train_masks.append(mask_path)

    # æ”¶é›†æ»‘å¡æµ‹è¯•æ•°æ®
    landslide_test_imgs = []
    landslide_test_dems = []
    landslide_test_masks = []

    for img_file in os.listdir(landslide_test_image_dir):
        if img_file.lower().endswith('.png'):
            img_path = os.path.join(landslide_test_image_dir, img_file)
            dem_path = os.path.join(landslide_test_dem_dir, img_file)
            mask_path = os.path.join(landslide_test_mask_dir, img_file)

            if os.path.exists(img_path) and os.path.exists(dem_path) and os.path.exists(mask_path):
                landslide_test_imgs.append(img_path)
                landslide_test_dems.append(dem_path)
                landslide_test_masks.append(mask_path)

    # æ”¶é›†éæ»‘å¡è®­ç»ƒæ•°æ®
    nonlandslide_train_imgs = []
    nonlandslide_train_dems = []

    for img_file in os.listdir(non_landslide_train_image_dir):
        if img_file.lower().endswith('.png'):
            img_path = os.path.join(non_landslide_train_image_dir, img_file)
            dem_path = os.path.join(non_landslide_train_dem_dir, img_file)

            if os.path.exists(img_path) and os.path.exists(dem_path):
                nonlandslide_train_imgs.append(img_path)
                nonlandslide_train_dems.append(dem_path)

    # æ”¶é›†éæ»‘å¡æµ‹è¯•æ•°æ®
    nonlandslide_test_imgs = []
    nonlandslide_test_dems = []

    for img_file in os.listdir(non_landslide_test_image_dir):
        if img_file.lower().endswith('.png'):
            img_path = os.path.join(non_landslide_test_image_dir, img_file)
            dem_path = os.path.join(non_landslide_test_dem_dir, img_file)

            if os.path.exists(img_path) and os.path.exists(dem_path):
                nonlandslide_test_imgs.append(img_path)
                nonlandslide_test_dems.append(dem_path)

    # åˆå¹¶æ­£è´Ÿæ ·æœ¬
    train_imgs = landslide_train_imgs + nonlandslide_train_imgs
    train_dems = landslide_train_dems + nonlandslide_train_dems
    # å…³é”®ä¿®æ”¹ï¼šæ— æ»‘å¡æ ·æœ¬çš„mask_pathè®¾ä¸ºNoneï¼Œæ•°æ®é›†ç±»ä¼šè‡ªåŠ¨åˆ›å»ºå…¨0 mask
    train_masks = landslide_train_masks + [None] * len(nonlandslide_train_imgs)

    test_imgs = landslide_test_imgs + nonlandslide_test_imgs
    test_dems = landslide_test_dems + nonlandslide_test_dems
    test_masks = landslide_test_masks + [None] * len(nonlandslide_test_imgs)

    print(f"è®­ç»ƒé›†: {len(train_imgs)} ä¸ªæ ·æœ¬")
    print(f"  - æœ‰æ»‘å¡: {len(landslide_train_imgs)} ({len(landslide_train_imgs) / len(train_imgs) * 100:.1f}%)")
    print(f"  - æ— æ»‘å¡: {len(nonlandslide_train_imgs)} ({len(nonlandslide_train_imgs) / len(train_imgs) * 100:.1f}%)")

    print(f"æµ‹è¯•é›†: {len(test_imgs)} ä¸ªæ ·æœ¬")
    print(f"  - æœ‰æ»‘å¡: {len(landslide_test_imgs)} ({len(landslide_test_imgs) / len(test_imgs) * 100:.1f}%)")
    print(f"  - æ— æ»‘å¡: {len(nonlandslide_test_imgs)} ({len(nonlandslide_test_imgs) / len(test_imgs) * 100:.1f}%)")

    # æ•°æ®å¢å¼ºé…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
    train_transform = A.Compose([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),
        A.OneOf([
            A.GaussNoise(var_limit=10.0, p=0.5),
            A.MotionBlur(blur_limit=3, p=0.5),
        ], p=0.2),
        A.Normalize(mean=(0.485, 0.456, 0.406, 0.5), std=(0.229, 0.224, 0.225, 0.25)),
        ToTensorV2()
    ])

    val_transform = A.Compose([
        A.Normalize(mean=(0.485, 0.456, 0.406, 0.5), std=(0.229, 0.224, 0.225, 0.25)),
        ToTensorV2()
    ])

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = LandslideDataset(
        image_paths=train_imgs,
        dem_paths=train_dems,
        mask_paths=train_masks,
        transform=train_transform,
        target_size=target_size
    )

    test_dataset = LandslideDataset(
        image_paths=test_imgs,
        dem_paths=test_dems,
        mask_paths=test_masks,
        transform=val_transform,
        target_size=target_size
    )

    return train_dataset, test_dataset


def split_dataset_with_balance(dataset, test_ratio=0.5, random_seed=42):
    """ä¿æŒæ»‘å¡æ ·æœ¬æ¯”ä¾‹çš„åˆ’åˆ†"""
    import numpy as np

    # è·å–æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾ï¼ˆæ˜¯å¦æœ‰æ»‘å¡ï¼‰
    labels = []
    for i in range(len(dataset)):
        _, _, mask, _ = dataset[i]
        labels.append(1 if mask.sum() > 0 else 0)

    labels = np.array(labels)
    indices = np.arange(len(dataset))

    # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬ç´¢å¼•
    pos_indices = indices[labels == 1]
    neg_indices = indices[labels == 0]

    # è®¾ç½®éšæœºç§å­
    np.random.seed(random_seed)
    np.random.shuffle(pos_indices)
    np.random.shuffle(neg_indices)

    # æŒ‰æ¯”ä¾‹åˆ’åˆ†æ­£æ ·æœ¬
    pos_test_size = int(len(pos_indices) * test_ratio)
    pos_val_indices = pos_indices[:pos_test_size]
    pos_test_indices = pos_indices[pos_test_size:]

    # æŒ‰æ¯”ä¾‹åˆ’åˆ†è´Ÿæ ·æœ¬
    neg_test_size = int(len(neg_indices) * test_ratio)
    neg_val_indices = neg_indices[:neg_test_size]
    neg_test_indices = neg_indices[neg_test_size:]

    # åˆå¹¶éªŒè¯é›†å’Œæµ‹è¯•é›†ç´¢å¼•
    val_indices = np.concatenate([pos_val_indices, neg_val_indices])
    test_indices = np.concatenate([pos_test_indices, neg_test_indices])

    # æ‰“ä¹±é¡ºåº
    np.random.shuffle(val_indices)
    np.random.shuffle(test_indices)

    # åˆ›å»ºå­é›†
    val_subset = torch.utils.data.Subset(dataset, val_indices)
    test_subset = torch.utils.data.Subset(dataset, test_indices)

    # ç»Ÿè®¡ä¿¡æ¯
    val_labels = labels[val_indices]
    test_labels = labels[test_indices]

    print(f"éªŒè¯é›†: {len(val_subset)} æ ·æœ¬ (æ»‘å¡: {val_labels.sum()}, éæ»‘å¡: {len(val_labels) - val_labels.sum()})")
    print(
        f"æµ‹è¯•é›†: {len(test_subset)} æ ·æœ¬ (æ»‘å¡: {test_labels.sum()}, éæ»‘å¡: {len(test_labels) - test_labels.sum()})")

    return val_subset, test_subset


def predict_and_evaluate(model, test_loader, device='cuda', save_dir='predictions', multigpu=False):
    """
    é€‚é…EarlyFusionNetçš„é¢„æµ‹è¯„ä¼°å‡½æ•°
    ä¿®æ”¹ï¼šæ”¯æŒ5ä¸ªè¿”å›å€¼çš„æ•°æ®åŠ è½½å™¨
    """
    import os
    import cv2
    import numpy as np
    import torch
    import matplotlib.pyplot as plt
    from tqdm import tqdm
    from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score

    os.makedirs(save_dir, exist_ok=True)

    if multigpu and torch.cuda.device_count() > 1:
        device_ids = list(range(torch.cuda.device_count()))
        print(f"ä½¿ç”¨å¤šGPUè¯„ä¼°: {device_ids}")
        model = nn.DataParallel(model, device_ids=device_ids)
        # è®¾ç½®ä¸»è®¾å¤‡
        if isinstance(device, str):
            device = torch.device(f'cuda:{device_ids[0]}')

    model = model.to(device)
    model.eval()

    model.eval()

    all_preds = []
    all_masks = []
    metrics = {'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': []}
    sample_results = []  # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„ç»“æœ

    with torch.no_grad():
        for i, batch in enumerate(tqdm(test_loader, desc='Testing')):
            # ===== ä¿®æ”¹è¿™é‡Œï¼šæ”¯æŒå¤šç§æ•°æ®æ ¼å¼ =====
            if len(batch) == 4:
                # æ ¼å¼: (optical, dem, mask, img_paths)
                optical, dem, mask, img_paths = batch
                is_landslide = None
            elif len(batch) == 5:
                # æ ¼å¼: (optical, dem, mask, is_landslide, img_paths)
                optical, dem, mask, is_landslide, img_paths = batch
            else:
                raise ValueError(f"æ„å¤–çš„batché•¿åº¦: {len(batch)}")
            # ===== ä¿®æ”¹ç»“æŸ =====

            optical = optical.to(device)
            dem = dem.to(device)
            mask = mask.cpu()  # åœ¨CPUä¸Šå¤„ç†mask

            # ä¿®æ”¹ç‚¹1: EarlyFusionNetç›´æ¥è¾“å‡ºlogits
            outputs = model(optical, dem)

            # ä¿®æ”¹ç‚¹2: é€šè¿‡sigmoidå¾—åˆ°æ¦‚ç‡ï¼Œç„¶åé˜ˆå€¼åŒ–
            pred_probs = torch.sigmoid(outputs).cpu()
            preds = (pred_probs > 0.5).float()

            # ä¿å­˜é¢„æµ‹ç»“æœ
            for j in range(len(img_paths)):
                img_name = os.path.basename(img_paths[j])
                # å»æ‰å¯èƒ½çš„æ‰©å±•å
                base_name = os.path.splitext(img_name)[0]

                # ä¿å­˜é¢„æµ‹æ©è†œ
                pred_mask = preds[j].squeeze().numpy()  # [H, W]
                pred_mask_uint8 = (pred_mask * 255).astype(np.uint8)

                # ä¿å­˜åŸå§‹é¢„æµ‹ï¼ˆæµ®ç‚¹æ•°æ¦‚ç‡ï¼‰
                pred_prob = pred_probs[j].squeeze().numpy()
                np.save(os.path.join(save_dir, f'prob_{base_name}.npy'), pred_prob)

                # ä¿å­˜äºŒå€¼åŒ–é¢„æµ‹
                cv2.imwrite(os.path.join(save_dir, f'pred_{base_name}.png'), pred_mask_uint8)

                # ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆå¦‚æœæœ‰çœŸå®æ©è†œï¼‰
                if mask[j].sum() > 0:
                    try:
                        # å°è¯•è¯»å–åŸå§‹å›¾åƒ
                        if os.path.exists(img_paths[j]):
                            orig_img = cv2.imread(img_paths[j])
                            if orig_img is not None:
                                orig_img = cv2.resize(orig_img, (256, 256))

                                # å¯è§†åŒ–
                                fig, axs = plt.subplots(1, 4, figsize=(20, 5))

                                # åŸå§‹å›¾åƒ
                                axs[0].imshow(cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB))
                                axs[0].set_title('Original Image')
                                axs[0].axis('off')

                                # DEMæ•°æ®ï¼ˆå¯é€‰ï¼‰
                                axs[1].imshow(dem[j].squeeze().cpu().numpy(), cmap='terrain')
                                axs[1].set_title('DEM Data')
                                axs[1].axis('off')

                                # çœŸå®æ©è†œ
                                axs[2].imshow(mask[j].squeeze().numpy(), cmap='gray')
                                axs[2].set_title('Ground Truth')
                                axs[2].axis('off')

                                # é¢„æµ‹ç»“æœ
                                axs[3].imshow(pred_prob, cmap='jet', vmin=0, vmax=1)
                                axs[3].set_title(f'Prediction')
                                axs[3].axis('off')

                                plt.tight_layout()
                                plt.savefig(os.path.join(save_dir, f'vis_{base_name}.png'),
                                            bbox_inches='tight', dpi=100)
                                plt.close()
                    except Exception as e:
                        print(f"å¯è§†åŒ– {img_name} æ—¶å‡ºé”™: {e}")

            # ä»…å¯¹æœ‰çœŸå®æ ‡ç­¾çš„æ ·æœ¬è®¡ç®—æŒ‡æ ‡
            valid_indices = [j for j in range(len(img_paths)) if mask[j].sum() > 0]
            if valid_indices:
                valid_preds = preds[valid_indices]
                valid_masks = mask[valid_indices]
                valid_names = [os.path.basename(img_paths[j]) for j in valid_indices]

                for idx, (pred, true, name) in enumerate(zip(valid_preds, valid_masks, valid_names)):
                    y_true = true.squeeze().numpy().flatten()
                    y_pred = pred.squeeze().numpy().flatten()

                    # äºŒå€¼åŒ–
                    y_true_bin = (y_true > 0.5).astype(int)
                    y_pred_bin = (y_pred > 0.5).astype(int)

                    # è®¡ç®—æŒ‡æ ‡
                    iou = jaccard_score(y_true_bin, y_pred_bin, zero_division=0)
                    precision = precision_score(y_true_bin, y_pred_bin, zero_division=0)
                    recall = recall_score(y_true_bin, y_pred_bin, zero_division=0)
                    f1 = f1_score(y_true_bin, y_pred_bin, zero_division=0)
                    accuracy = np.mean(y_true_bin == y_pred_bin)

                    # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
                    metrics['iou'].append(iou)
                    metrics['precision'].append(precision)
                    metrics['recall'].append(recall)
                    metrics['f1'].append(f1)
                    metrics['accuracy'].append(accuracy)

                    # è®°å½•æ ·æœ¬ç»“æœ
                    sample_results.append({
                        'image': name,
                        'iou': iou,
                        'precision': precision,
                        'recall': recall,
                        'f1': f1,
                        'accuracy': accuracy,
                        'true_positives': np.sum((y_true_bin == 1) & (y_pred_bin == 1)),
                        'false_positives': np.sum((y_true_bin == 0) & (y_pred_bin == 1)),
                        'false_negatives': np.sum((y_true_bin == 1) & (y_pred_bin == 0)),
                        'true_negatives': np.sum((y_true_bin == 0) & (y_pred_bin == 0))
                    })

    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    if metrics['iou']:
        print("\n" + "=" * 60)
        print("æ€»ä½“è¯„ä¼°ç»“æœ:")
        print("=" * 60)

        for metric in ['iou', 'precision', 'recall', 'f1', 'accuracy']:
            values = metrics[metric]
            if values:
                print(f"{metric.upper():12s}: {np.mean(values):.4f} Â± {np.std(values):.4f}")
                print(f"  èŒƒå›´: [{np.min(values):.4f}, {np.max(values):.4f}]")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        import pandas as pd
        df_results = pd.DataFrame(sample_results)
        df_results.to_csv(os.path.join(save_dir, 'detailed_results.csv'), index=False)

        # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
        summary_stats = {
            'metric': ['iou', 'precision', 'recall', 'f1', 'accuracy'],
            'mean': [np.mean(metrics[m]) for m in ['iou', 'precision', 'recall', 'f1', 'accuracy']],
            'std': [np.std(metrics[m]) for m in ['iou', 'precision', 'recall', 'f1', 'accuracy']],
            'min': [np.min(metrics[m]) for m in ['iou', 'precision', 'recall', 'f1', 'accuracy']],
            'max': [np.max(metrics[m]) for m in ['iou', 'precision', 'recall', 'f1', 'accuracy']]
        }
        pd.DataFrame(summary_stats).to_csv(os.path.join(save_dir, 'summary_stats.csv'), index=False)

        # æ··æ·†çŸ©é˜µæ€»è®¡
        total_tp = sum([r['true_positives'] for r in sample_results])
        total_fp = sum([r['false_positives'] for r in sample_results])
        total_fn = sum([r['false_negatives'] for r in sample_results])
        total_tn = sum([r['true_negatives'] for r in sample_results])

        print("\næ··æ·†çŸ©é˜µæ€»è®¡:")
        print(f"True Positives:  {total_tp}")
        print(f"False Positives: {total_fp}")
        print(f"False Negatives: {total_fn}")
        print(f"True Negatives:  {total_tn}")

        # ä»æ€»è®¡è®¡ç®—å®è§‚æŒ‡æ ‡
        macro_precision = total_tp / (total_tp + total_fp + 1e-10)
        macro_recall = total_tp / (total_tp + total_fn + 1e-10)
        macro_f1 = 2 * macro_precision * macro_recall / (macro_precision + macro_recall + 1e-10)
        macro_iou = total_tp / (total_tp + total_fp + total_fn + 1e-10)

        print("\nå®è§‚æŒ‡æ ‡ï¼ˆä»æ€»è®¡è®¡ç®—ï¼‰:")
        print(f"Macro IoU:       {macro_iou:.4f}")
        print(f"Macro Precision: {macro_precision:.4f}")
        print(f"Macro Recall:    {macro_recall:.4f}")
        print(f"Macro F1:        {macro_f1:.4f}")

        results = {
            'micro_iou': np.mean(metrics['iou']),
            'micro_precision': np.mean(metrics['precision']),
            'micro_recall': np.mean(metrics['recall']),
            'micro_f1': np.mean(metrics['f1']),
            'macro_iou': macro_iou,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'sample_count': len(sample_results)
        }
    else:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„çœŸå€¼æ©è†œè¿›è¡Œè¯„ä¼°")
        results = {}

    return results


def visualize_predictions_comparison(model, test_loader, device='cuda', num_samples=5):
    """
    å¯è§†åŒ–é¢„æµ‹å¯¹æ¯”ï¼ˆå•ç‹¬å‡½æ•°ï¼Œæ›´æ¸…æ™°ï¼‰
    """
    import matplotlib.pyplot as plt

    model.eval()

    with torch.no_grad():
        for i, (optical, dem, mask, img_paths) in enumerate(test_loader):
            if i >= 1:  # åªå–ç¬¬ä¸€ä¸ªbatch
                break

            optical = optical.to(device)
            dem = dem.to(device)

            outputs = model(optical, dem)
            pred_probs = torch.sigmoid(outputs).cpu()

            # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬
            num_show = min(num_samples, len(optical))

            fig, axes = plt.subplots(num_show, 4, figsize=(16, num_show * 4))
            if num_show == 1:
                axes = axes.reshape(1, -1)

            for idx in range(num_show):
                # å…‰å­¦å›¾åƒ
                axes[idx, 0].imshow(optical[idx].cpu().permute(1, 2, 0).numpy())
                axes[idx, 0].set_title('Optical Image')
                axes[idx, 0].axis('off')

                # DEMæ•°æ®
                axes[idx, 1].imshow(dem[idx].cpu().squeeze().numpy(), cmap='terrain')
                axes[idx, 1].set_title('DEM Data')
                axes[idx, 1].axis('off')

                # çœŸå®æ©è†œ
                if mask[idx].sum() > 0:
                    axes[idx, 2].imshow(mask[idx].squeeze().numpy(), cmap='gray')
                axes[idx, 2].set_title('Ground Truth')
                axes[idx, 2].axis('off')

                # é¢„æµ‹ç»“æœ
                pred_prob = pred_probs[idx].squeeze().numpy()
                im = axes[idx, 3].imshow(pred_prob, cmap='jet', vmin=0, vmax=1)
                axes[idx, 3].set_title('Prediction')
                axes[idx, 3].axis('off')

                # æ·»åŠ é¢œè‰²æ¡
                plt.colorbar(im, ax=axes[idx, 3], fraction=0.046, pad=0.04)

            plt.tight_layout()
            plt.savefig('predictions_comparison.png', dpi=150, bbox_inches='tight')
            plt.show()
            break



class RSU7(nn.Module):
    """RSU-7æ¨¡å—: é«˜åº¦ä¸º7çš„æ®‹å·®Uå—"""

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()
        self.out_ch = out_ch

        # ç¼–ç å™¨éƒ¨åˆ†
        self.conv0 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)

        self.conv1 = nn.Conv2d(out_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_ch)

        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv2 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_ch)

        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv3 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(mid_ch)

        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv4 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn4 = nn.BatchNorm2d(mid_ch)

        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv5 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn5 = nn.BatchNorm2d(mid_ch)

        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv6 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn6 = nn.BatchNorm2d(mid_ch)

        # æœ€åº•å±‚çš„å·ç§¯
        self.conv7 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, dilation=2, padding=2, bias=False)
        self.bn7 = nn.BatchNorm2d(mid_ch)

        # è§£ç å™¨éƒ¨åˆ†
        self.conv6d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn6d = nn.BatchNorm2d(mid_ch)

        self.conv5d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn5d = nn.BatchNorm2d(mid_ch)

        self.conv4d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn4d = nn.BatchNorm2d(mid_ch)

        self.conv3d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3d = nn.BatchNorm2d(mid_ch)

        self.conv2d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2d = nn.BatchNorm2d(mid_ch)

        self.conv1d = nn.Conv2d(mid_ch * 2, out_ch, kernel_size=3, padding=1, bias=False)
        self.bn1d = nn.BatchNorm2d(out_ch)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        x_input = x

        # ç¬¬ä¸€å±‚å·ç§¯
        hx = self.conv0(x_input)
        hx_in = hx  # ä¿å­˜ç”¨äºæ®‹å·®è¿æ¥

        # ç¼–ç å™¨è·¯å¾„
        hx1 = self.relu(self.bn1(self.conv1(hx)))
        hx = self.pool1(hx1)

        hx2 = self.relu(self.bn2(self.conv2(hx)))
        hx = self.pool2(hx2)

        hx3 = self.relu(self.bn3(self.conv3(hx)))
        hx = self.pool3(hx3)

        hx4 = self.relu(self.bn4(self.conv4(hx)))
        hx = self.pool4(hx4)

        hx5 = self.relu(self.bn5(self.conv5(hx)))
        hx = self.pool5(hx5)

        hx6 = self.relu(self.bn6(self.conv6(hx)))

        hx7 = self.relu(self.bn7(self.conv7(hx6)))

        # è§£ç å™¨è·¯å¾„
        hx6d = self.relu(self.bn6d(self.conv6d(torch.cat((hx6, hx7), 1))))
        hx6dup = F.interpolate(hx6d, size=hx5.shape[2:], mode='bilinear', align_corners=True)

        hx5d = self.relu(self.bn5d(self.conv5d(torch.cat((hx5, hx6dup), 1))))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=True)

        hx4d = self.relu(self.bn4d(self.conv4d(torch.cat((hx4, hx5dup), 1))))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=True)

        hx3d = self.relu(self.bn3d(self.conv3d(torch.cat((hx3, hx4dup), 1))))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=True)

        hx2d = self.relu(self.bn2d(self.conv2d(torch.cat((hx2, hx3dup), 1))))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=True)

        hx1d = self.relu(self.bn1d(self.conv1d(torch.cat((hx1, hx2dup), 1))))

        # ç¡®ä¿hx1då’Œhx_inå¤§å°ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™è°ƒæ•´hx_in
        if hx1d.shape != hx_in.shape:
            # è°ƒæ•´hx_inçš„å¤§å°ä»¥åŒ¹é…hx1d
            hx_in_adjusted = F.interpolate(hx_in, size=hx1d.shape[2:], mode='bilinear', align_corners=True)
        else:
            hx_in_adjusted = hx_in

        # æ®‹å·®è¿æ¥
        return hx1d + hx_in_adjusted


class RSU6(nn.Module):
    """RSU-6æ¨¡å—: é«˜åº¦ä¸º6çš„æ®‹å·®Uå—"""

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU6, self).__init__()
        self.out_ch = out_ch

        self.conv0 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)

        self.conv1 = nn.Conv2d(out_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_ch)

        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv2 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_ch)

        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv3 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(mid_ch)

        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv4 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn4 = nn.BatchNorm2d(mid_ch)

        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv5 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn5 = nn.BatchNorm2d(mid_ch)

        self.conv6 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, dilation=2, padding=2, bias=False)
        self.bn6 = nn.BatchNorm2d(mid_ch)

        # è§£ç å™¨
        self.conv5d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn5d = nn.BatchNorm2d(mid_ch)

        self.conv4d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn4d = nn.BatchNorm2d(mid_ch)

        self.conv3d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3d = nn.BatchNorm2d(mid_ch)

        self.conv2d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2d = nn.BatchNorm2d(mid_ch)

        self.conv1d = nn.Conv2d(mid_ch * 2, out_ch, kernel_size=3, padding=1, bias=False)
        self.bn1d = nn.BatchNorm2d(out_ch)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        x_input = x

        # ç¬¬ä¸€å±‚å·ç§¯
        hx = self.conv0(x_input)
        hx_in = hx  # ä¿å­˜ç”¨äºæ®‹å·®è¿æ¥

        hx1 = self.relu(self.bn1(self.conv1(hx)))
        hx = self.pool1(hx1)

        hx2 = self.relu(self.bn2(self.conv2(hx)))
        hx = self.pool2(hx2)

        hx3 = self.relu(self.bn3(self.conv3(hx)))
        hx = self.pool3(hx3)

        hx4 = self.relu(self.bn4(self.conv4(hx)))
        hx = self.pool4(hx4)

        hx5 = self.relu(self.bn5(self.conv5(hx)))

        hx6 = self.relu(self.bn6(self.conv6(hx5)))

        hx5d = self.relu(self.bn5d(self.conv5d(torch.cat((hx5, hx6), 1))))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=True)

        hx4d = self.relu(self.bn4d(self.conv4d(torch.cat((hx4, hx5dup), 1))))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=True)

        hx3d = self.relu(self.bn3d(self.conv3d(torch.cat((hx3, hx4dup), 1))))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=True)

        hx2d = self.relu(self.bn2d(self.conv2d(torch.cat((hx2, hx3dup), 1))))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=True)

        hx1d = self.relu(self.bn1d(self.conv1d(torch.cat((hx1, hx2dup), 1))))

        # ç¡®ä¿hx1då’Œhx_inå¤§å°ä¸€è‡´
        if hx1d.shape != hx_in.shape:
            hx_in_adjusted = F.interpolate(hx_in, size=hx1d.shape[2:], mode='bilinear', align_corners=True)
        else:
            hx_in_adjusted = hx_in

        # æ®‹å·®è¿æ¥
        return hx1d + hx_in_adjusted


class RSU5(nn.Module):
    """RSU-5æ¨¡å—: é«˜åº¦ä¸º5çš„æ®‹å·®Uå—"""

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU5, self).__init__()
        self.out_ch = out_ch

        self.conv0 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)

        self.conv1 = nn.Conv2d(out_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_ch)

        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv2 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_ch)

        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv3 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(mid_ch)

        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv4 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn4 = nn.BatchNorm2d(mid_ch)

        self.conv5 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, dilation=2, padding=2, bias=False)
        self.bn5 = nn.BatchNorm2d(mid_ch)

        # è§£ç å™¨
        self.conv4d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn4d = nn.BatchNorm2d(mid_ch)

        self.conv3d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3d = nn.BatchNorm2d(mid_ch)

        self.conv2d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2d = nn.BatchNorm2d(mid_ch)

        self.conv1d = nn.Conv2d(mid_ch * 2, out_ch, kernel_size=3, padding=1, bias=False)
        self.bn1d = nn.BatchNorm2d(out_ch)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        x_input = x

        # ç¬¬ä¸€å±‚å·ç§¯
        hx = self.conv0(x_input)
        hx_in = hx  # ä¿å­˜ç”¨äºæ®‹å·®è¿æ¥

        hx1 = self.relu(self.bn1(self.conv1(hx)))
        hx = self.pool1(hx1)

        hx2 = self.relu(self.bn2(self.conv2(hx)))
        hx = self.pool2(hx2)

        hx3 = self.relu(self.bn3(self.conv3(hx)))
        hx = self.pool3(hx3)

        hx4 = self.relu(self.bn4(self.conv4(hx)))

        hx5 = self.relu(self.bn5(self.conv5(hx4)))

        hx4d = self.relu(self.bn4d(self.conv4d(torch.cat((hx4, hx5), 1))))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=True)

        hx3d = self.relu(self.bn3d(self.conv3d(torch.cat((hx3, hx4dup), 1))))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=True)

        hx2d = self.relu(self.bn2d(self.conv2d(torch.cat((hx2, hx3dup), 1))))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=True)

        hx1d = self.relu(self.bn1d(self.conv1d(torch.cat((hx1, hx2dup), 1))))

        # ç¡®ä¿hx1då’Œhx_inå¤§å°ä¸€è‡´
        if hx1d.shape != hx_in.shape:
            hx_in_adjusted = F.interpolate(hx_in, size=hx1d.shape[2:], mode='bilinear', align_corners=True)
        else:
            hx_in_adjusted = hx_in

        # æ®‹å·®è¿æ¥
        return hx1d + hx_in_adjusted


class RSU4(nn.Module):
    """RSU-4æ¨¡å—: é«˜åº¦ä¸º4çš„æ®‹å·®Uå—"""

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU4, self).__init__()
        self.out_ch = out_ch

        self.conv0 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)

        self.conv1 = nn.Conv2d(out_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_ch)

        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv2 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_ch)

        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.conv3 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(mid_ch)

        self.conv4 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, dilation=2, padding=2, bias=False)
        self.bn4 = nn.BatchNorm2d(mid_ch)

        # è§£ç å™¨
        self.conv3d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3d = nn.BatchNorm2d(mid_ch)

        self.conv2d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2d = nn.BatchNorm2d(mid_ch)

        self.conv1d = nn.Conv2d(mid_ch * 2, out_ch, kernel_size=3, padding=1, bias=False)
        self.bn1d = nn.BatchNorm2d(out_ch)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        x_input = x

        # ç¬¬ä¸€å±‚å·ç§¯
        hx = self.conv0(x_input)
        hx_in = hx  # ä¿å­˜ç”¨äºæ®‹å·®è¿æ¥

        hx1 = self.relu(self.bn1(self.conv1(hx)))
        hx = self.pool1(hx1)

        hx2 = self.relu(self.bn2(self.conv2(hx)))
        hx = self.pool2(hx2)

        hx3 = self.relu(self.bn3(self.conv3(hx)))

        hx4 = self.relu(self.bn4(self.conv4(hx3)))

        hx3d = self.relu(self.bn3d(self.conv3d(torch.cat((hx3, hx4), 1))))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=True)

        hx2d = self.relu(self.bn2d(self.conv2d(torch.cat((hx2, hx3dup), 1))))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=True)

        hx1d = self.relu(self.bn1d(self.conv1d(torch.cat((hx1, hx2dup), 1))))

        # ç¡®ä¿hx1då’Œhx_inå¤§å°ä¸€è‡´
        if hx1d.shape != hx_in.shape:
            hx_in_adjusted = F.interpolate(hx_in, size=hx1d.shape[2:], mode='bilinear', align_corners=True)
        else:
            hx_in_adjusted = hx_in

        # æ®‹å·®è¿æ¥
        return hx1d + hx_in_adjusted


class RSU4F(nn.Module):
    """RSU-4Fæ¨¡å—: æ— ä¸‹é‡‡æ ·çš„RSU-4ï¼ˆä½¿ç”¨ç©ºæ´å·ç§¯ï¼‰"""

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU4F, self).__init__()
        self.out_ch = out_ch

        self.conv0 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)

        self.conv1 = nn.Conv2d(out_ch, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_ch)

        self.conv2 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, dilation=2, padding=2, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_ch)

        self.conv3 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, dilation=4, padding=4, bias=False)
        self.bn3 = nn.BatchNorm2d(mid_ch)

        self.conv4 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, dilation=8, padding=8, bias=False)
        self.bn4 = nn.BatchNorm2d(mid_ch)

        # è§£ç å™¨
        self.conv3d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn3d = nn.BatchNorm2d(mid_ch)

        self.conv2d = nn.Conv2d(mid_ch * 2, mid_ch, kernel_size=3, padding=1, bias=False)
        self.bn2d = nn.BatchNorm2d(mid_ch)

        self.conv1d = nn.Conv2d(mid_ch * 2, out_ch, kernel_size=3, padding=1, bias=False)
        self.bn1d = nn.BatchNorm2d(out_ch)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        x_input = x

        # ç¬¬ä¸€å±‚å·ç§¯
        hx = self.conv0(x_input)
        hx_in = hx  # ä¿å­˜ç”¨äºæ®‹å·®è¿æ¥

        hx1 = self.relu(self.bn1(self.conv1(hx)))
        hx2 = self.relu(self.bn2(self.conv2(hx1)))
        hx3 = self.relu(self.bn3(self.conv3(hx2)))
        hx4 = self.relu(self.bn4(self.conv4(hx3)))

        hx3d = self.relu(self.bn3d(self.conv3d(torch.cat((hx3, hx4), 1))))
        hx2d = self.relu(self.bn2d(self.conv2d(torch.cat((hx2, hx3d), 1))))
        hx1d = self.relu(self.bn1d(self.conv1d(torch.cat((hx1, hx2d), 1))))

        # RSU4Fæ²¡æœ‰ä¸‹é‡‡æ ·ï¼Œæ‰€ä»¥å°ºå¯¸åº”è¯¥ä¿æŒä¸å˜
        # æ®‹å·®è¿æ¥
        return hx1d + hx_in


class U2NET(nn.Module):
    """U^2-Netæ¨¡å‹ - æ—©æœŸèåˆç‰ˆæœ¬ï¼Œè¾“å…¥è¾“å‡ºä¸åŸå§‹U-Netä¿æŒä¸€è‡´"""

    def __init__(self, n_channels=4, n_classes=1):
        super(U2NET, self).__init__()

        # ç¼–ç å™¨ (RSUæ¨¡å—)
        self.stage1 = RSU7(n_channels, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage2 = RSU6(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage3 = RSU5(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage4 = RSU4(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage5 = RSU4F(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage6 = RSU4F(512, 256, 512)

        # è§£ç å™¨
        self.stage5d = RSU4F(1024, 256, 512)
        self.stage4d = RSU4(1024, 128, 256)
        self.stage3d = RSU5(512, 64, 128)
        self.stage2d = RSU6(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)

        # ä¾§è¾¹è¾“å‡º
        self.side1 = nn.Conv2d(64, n_classes, kernel_size=3, padding=1)
        self.side2 = nn.Conv2d(64, n_classes, kernel_size=3, padding=1)
        self.side3 = nn.Conv2d(128, n_classes, kernel_size=3, padding=1)
        self.side4 = nn.Conv2d(256, n_classes, kernel_size=3, padding=1)
        self.side5 = nn.Conv2d(512, n_classes, kernel_size=3, padding=1)
        self.side6 = nn.Conv2d(512, n_classes, kernel_size=3, padding=1)

        # æœ€ç»ˆèåˆå±‚
        self.outconv = nn.Conv2d(6 * n_classes, n_classes, kernel_size=1)

    def forward(self, optical, dem):
        # æ—©æœŸèåˆ: åœ¨é€šé“ç»´åº¦æ‹¼æ¥
        x = torch.cat([optical, dem], dim=1)

        # ç¼–ç è·¯å¾„
        hx1 = self.stage1(x)
        hx = self.pool12(hx1)

        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)

        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)

        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)

        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)

        hx6 = self.stage6(hx)
        hx6up = F.interpolate(hx6, size=hx5.shape[2:], mode='bilinear', align_corners=True)

        # è§£ç è·¯å¾„
        hx5d = self.stage5d(torch.cat((hx6up, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=True)

        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=True)

        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=True)

        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=True)

        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))

        # ä¾§è¾¹è¾“å‡º
        d1 = self.side1(hx1d)

        d2 = self.side2(hx2d)
        d2 = F.interpolate(d2, size=x.shape[2:], mode='bilinear', align_corners=True)

        d3 = self.side3(hx3d)
        d3 = F.interpolate(d3, size=x.shape[2:], mode='bilinear', align_corners=True)

        d4 = self.side4(hx4d)
        d4 = F.interpolate(d4, size=x.shape[2:], mode='bilinear', align_corners=True)

        d5 = self.side5(hx5d)
        d5 = F.interpolate(d5, size=x.shape[2:], mode='bilinear', align_corners=True)

        d6 = self.side6(hx6)
        d6 = F.interpolate(d6, size=x.shape[2:], mode='bilinear', align_corners=True)

        # èåˆæ‰€æœ‰ä¾§è¾¹è¾“å‡º
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))

        return d0



def get_simple_training_config():
    """è·å–ç®€å•è®­ç»ƒé…ç½®"""

    # 1. åˆ›å»ºç®€å•æ¨¡å‹
    model = U2NET(n_channels=4, n_classes=1)

    # 2. ä½¿ç”¨æ ‡å‡†æŸå¤±å‡½æ•°ï¼ˆå…ˆæ’é™¤å¤æ‚çš„æŸå¤±å‡½æ•°ï¼‰
    def simple_loss(pred, target):
        """ç®€å•çš„BCEæŸå¤±å‡½æ•°"""
        return nn.BCEWithLogitsLoss()(pred, target)

    # æˆ–è€…è”åˆæŸå¤±
    def combined_loss(pred, target):
        """BCE + DiceæŸå¤±"""
        bce = nn.BCEWithLogitsLoss()(pred, target)

        # DiceæŸå¤±
        probs = torch.sigmoid(pred)
        smooth = 1e-6
        intersection = (probs * target).sum(dim=(1, 2, 3))
        union = probs.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))
        dice = (2. * intersection + smooth) / (union + smooth)
        dice_loss = 1 - dice.mean()

        return bce + dice_loss

    def combined_loss_v1(pred, target, alpha=0.25, gamma=2.0, dice_weight=0.5):
        """
        Focal Loss + Dice Loss
        ä¼˜ç‚¹ï¼šè‡ªåŠ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Œå¯¹ç®€å•æ ·æœ¬é™æƒ
        é€‚åˆï¼šFPè¿‡å¤šï¼Œæ­£è´Ÿæ ·æœ¬æä¸å¹³è¡¡çš„æƒ…å†µ
        """
        # Focal Losséƒ¨åˆ†
        bce_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = alpha * (1 - pt) ** gamma * bce_loss
        focal_loss = focal_loss.mean()

        # Dice Losséƒ¨åˆ†
        probs = torch.sigmoid(pred)
        smooth = 1e-6
        intersection = (probs * target).sum(dim=(1, 2, 3))
        union = probs.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))
        dice = (2. * intersection + smooth) / (union + smooth)
        dice_loss = 1 - dice.mean()

        return focal_loss + dice_weight * dice_loss

    # 3. ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-4,
        weight_decay=1e-4
    )

    # 4. å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=5,
    )

    return model, combined_loss, optimizer, scheduler


def load_model_with_multigpu_support(model, model_path):
    """
    åŠ è½½æ¨¡å‹ï¼Œè‡ªåŠ¨å¤„ç†å¤šGPUè®­ç»ƒçš„æƒé‡

    å‚æ•°:
        model: æ¨¡å‹å®ä¾‹
        model_path: æƒé‡æ–‡ä»¶è·¯å¾„

    è¿”å›:
        model: åŠ è½½æƒé‡åçš„æ¨¡å‹
    """
    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location='cpu')

    # æå–state_dict
    if isinstance(checkpoint, dict):
        # æ£€æŸ¥æ˜¯å®Œæ•´checkpointè¿˜æ˜¯ç›´æ¥state_dict
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
    else:
        state_dict = checkpoint

    # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šGPUæƒé‡
    if any(key.startswith('module.') for key in state_dict.keys()):
        print("ğŸ”„ å¤„ç†å¤šGPUè®­ç»ƒæƒé‡...")
        # ç§»é™¤'module.'å‰ç¼€
        new_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('module.'):
                new_key = key[7:]  # å»æ‰'module.'
            else:
                new_key = key
            new_state_dict[new_key] = value
        state_dict = new_state_dict

    # åŠ è½½æƒé‡
    model.load_state_dict(state_dict)
    model.eval()

    print(f" æ¨¡å‹æƒé‡å·²åŠ è½½ï¼ˆ{len(state_dict)}ä¸ªå‚æ•°ï¼‰")
    return model


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""

    # è·å–é…ç½®
    model, criterion, optimizer, scheduler = get_simple_training_config()

    print(f"æ¨¡å‹æ¶æ„: {model.__class__.__name__}")
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")

    device_ids = list(range(torch.cuda.device_count()))
    print(f"å¯ç”¨çš„GPU: {device_ids}")

    # è®¾ç½®
    set_seed(42)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ•°æ®å‡†å¤‡ï¼ˆä½¿ç”¨æ–°å‡½æ•°ï¼‰
    data_dir = r"D:\ly\landsint\Bijie-landslide-dataset"
    train_dataset, test_dataset = prepare_datasets_with_masks(data_dir, target_size=(256, 256))

    # åˆ’åˆ†éªŒè¯é›†
    val_ratio = 0.5
    val_size = int(len(test_dataset) * val_ratio)
    test_size = len(test_dataset) - val_size

    test_subset, val_subset = torch.utils.data.random_split(
        test_dataset, [test_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_subset)} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(test_subset)} æ ·æœ¬")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 8
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)

    model.load_state_dict(torch.load(r'D:\ly\landsint\result\final_U2NET_model.pth', map_location=torch.device('cpu')))

    model.eval()

    # 2. è¿è¡Œè¯„ä¼°
    results = predict_and_evaluate(
        model=model,
        test_loader=test_loader,  # ä½ çš„æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device='cpu',
        save_dir=r'D:\ly\landsint\result\predictions_results_U2net',
        multigpu=True
    )


# è¿è¡Œè°ƒè¯•
if __name__ == "__main__":
    main()

